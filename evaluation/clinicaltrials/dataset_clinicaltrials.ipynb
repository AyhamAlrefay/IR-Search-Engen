{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import ir_datasets\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import unicodedata\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# %load_ext autotime\n",
    "dataset = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    " \n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(word_tokenize(str(sentence)))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def remove_urls(data):\n",
    "    cleaned_text = re.sub(r'/(https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z]{2,}(\\.[a-zA-Z]{2,})(\\.[a-zA-Z]{2,})?\\/[a-zA-Z0-9]{2,}|((https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z]{2,}(\\.[a-zA-Z]{2,})(\\.[a-zA-Z]{2,})?)|(https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z0-9]{2,}\\.[a-zA-Z0-9]{2,}\\.[a-zA-Z0-9]{2,}(\\.[a-zA-Z0-9]{2,})?/g', '', data)\n",
    "    return cleaned_text;\n",
    "\n",
    "def replace_contractions(data):\n",
    "    return \" \".join(contractions.fix(data))\n",
    "\n",
    "def correct_sentence_spelling(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in misspelled:\n",
    "            corrected = spell.correction(token)\n",
    "            if corrected is not None:\n",
    "                tokens[i] = corrected\n",
    "    return \" \".join( tokens)\n",
    "\n",
    "def custom_tokenizer(text: str) -> List[str]:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) \n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = stemming(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) \n",
    "    data = remove_punctuation(data)\n",
    "    data = remove_stop_words(data) \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter=0\n",
    "corpus = {}\n",
    "for doc in dataset.docs_iter():\n",
    "    corpus[doc.doc_id] = doc.condition\n",
    "\n",
    "documents = list(corpus.values())\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer setup\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, preprocessor=preprocess)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=vectorizer.get_feature_names_out(), index=corpus.keys())\n",
    "tfidf_model = vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID:  1, Recall@10: 0.05917159763313609\n",
      "Query ID: 1, Precision@10: 1.0\n",
      "Query ID:  2, Recall@10: 0.02962962962962963\n",
      "Query ID: 2, Precision@10: 0.8\n",
      "Query ID:  3, Recall@10: 0.03571428571428571\n",
      "Query ID: 3, Precision@10: 0.3\n",
      "Query ID:  4, Recall@10: 0.042105263157894736\n",
      "Query ID: 4, Precision@10: 0.4\n",
      "Query ID:  5, Recall@10: 0.03980099502487562\n",
      "Query ID: 5, Precision@10: 0.8\n",
      "Query ID:  6, Recall@10: 0.0380952380952381\n",
      "Query ID: 6, Precision@10: 0.8\n",
      "Query ID:  7, Recall@10: 0.043478260869565216\n",
      "Query ID: 7, Precision@10: 0.7\n",
      "Query ID:  8, Recall@10: 0.0457516339869281\n",
      "Query ID: 8, Precision@10: 0.7\n",
      "Query ID:  9, Recall@10: 0.037383177570093455\n",
      "Query ID: 9, Precision@10: 0.8\n",
      "Query ID:  10, Recall@10: 0.0\n",
      "Query ID: 10, Precision@10: 0.0\n",
      "Query ID:  11, Recall@10: 0.07142857142857142\n",
      "Query ID: 11, Precision@10: 0.9\n",
      "Query ID:  12, Recall@10: 0.04697986577181208\n",
      "Query ID: 12, Precision@10: 0.7\n",
      "Query ID:  13, Recall@10: 0.028985507246376812\n",
      "Query ID: 13, Precision@10: 0.4\n",
      "Query ID:  14, Recall@10: 0.003401360544217687\n",
      "Query ID: 14, Precision@10: 0.1\n",
      "Query ID:  15, Recall@10: 0.023076923076923078\n",
      "Query ID: 15, Precision@10: 0.6\n",
      "Query ID:  16, Recall@10: 0.021739130434782608\n",
      "Query ID: 16, Precision@10: 0.3\n",
      "Query ID:  17, Recall@10: 0.03187250996015936\n",
      "Query ID: 17, Precision@10: 0.8\n",
      "Query ID:  18, Recall@10: 0.02912621359223301\n",
      "Query ID: 18, Precision@10: 0.6\n",
      "Query ID:  19, Recall@10: 0.0457516339869281\n",
      "Query ID: 19, Precision@10: 0.7\n",
      "Query ID:  20, Recall@10: 0.11428571428571428\n",
      "Query ID: 20, Precision@10: 0.8\n",
      "Query ID:  21, Recall@10: 0.0\n",
      "Query ID: 21, Precision@10: 0.0\n",
      "Query ID:  22, Recall@10: 0.043795620437956206\n",
      "Query ID: 22, Precision@10: 0.6\n",
      "Query ID:  23, Recall@10: 0.026217228464419477\n",
      "Query ID: 23, Precision@10: 0.7\n",
      "Query ID:  24, Recall@10: 0.03164556962025317\n",
      "Query ID: 24, Precision@10: 0.5\n",
      "Query ID:  25, Recall@10: 0.0392156862745098\n",
      "Query ID: 25, Precision@10: 0.4\n",
      "Query ID:  26, Recall@10: 0.04375\n",
      "Query ID: 26, Precision@10: 0.7\n",
      "Query ID:  27, Recall@10: 0.02575107296137339\n",
      "Query ID: 27, Precision@10: 0.6\n",
      "Query ID:  28, Recall@10: 0.018518518518518517\n",
      "Query ID: 28, Precision@10: 0.6\n",
      "Query ID:  29, Recall@10: 0.015503875968992248\n",
      "Query ID: 29, Precision@10: 0.4\n",
      "Query ID:  30, Recall@10: 0.04\n",
      "Query ID: 30, Precision@10: 0.6\n",
      "Query ID:  31, Recall@10: 0.029069767441860465\n",
      "Query ID: 31, Precision@10: 0.5\n",
      "Query ID:  32, Recall@10: 0.03773584905660377\n",
      "Query ID: 32, Precision@10: 0.2\n",
      "Query ID:  33, Recall@10: 0.009569377990430622\n",
      "Query ID: 33, Precision@10: 0.2\n",
      "Query ID:  34, Recall@10: 0.006666666666666667\n",
      "Query ID: 34, Precision@10: 0.1\n",
      "Query ID:  35, Recall@10: 0.017241379310344827\n",
      "Query ID: 35, Precision@10: 0.3\n",
      "Query ID:  36, Recall@10: 0.006472491909385114\n",
      "Query ID: 36, Precision@10: 0.2\n",
      "Query ID:  37, Recall@10: 0.036290322580645164\n",
      "Query ID: 37, Precision@10: 0.9\n",
      "Query ID:  38, Recall@10: 0.1111111111111111\n",
      "Query ID: 38, Precision@10: 0.9\n",
      "Query ID:  39, Recall@10: 0.0\n",
      "Query ID: 39, Precision@10: 0.0\n",
      "Query ID:  40, Recall@10: 0.23076923076923078\n",
      "Query ID: 40, Precision@10: 0.3\n",
      "Query ID:  41, Recall@10: 0.022727272727272728\n",
      "Query ID: 41, Precision@10: 0.7\n",
      "Query ID:  42, Recall@10: 0.0\n",
      "Query ID: 42, Precision@10: 0.0\n",
      "Query ID:  43, Recall@10: 0.04516129032258064\n",
      "Query ID: 43, Precision@10: 0.7\n",
      "Query ID:  44, Recall@10: 0.0\n",
      "Query ID: 44, Precision@10: 0.0\n",
      "Query ID:  45, Recall@10: 0.04838709677419355\n",
      "Query ID: 45, Precision@10: 0.9\n",
      "Query ID:  46, Recall@10: 0.08108108108108109\n",
      "Query ID: 46, Precision@10: 0.9\n",
      "Query ID:  47, Recall@10: 0.011278195488721804\n",
      "Query ID: 47, Precision@10: 0.3\n",
      "Query ID:  48, Recall@10: 0.15151515151515152\n",
      "Query ID: 48, Precision@10: 1.0\n",
      "Query ID:  49, Recall@10: 0.09090909090909091\n",
      "Query ID: 49, Precision@10: 0.7\n",
      "Query ID:  50, Recall@10: 0.30434782608695654\n",
      "Query ID: 50, Precision@10: 0.7\n",
      "Query ID:  51, Recall@10: 0.07042253521126761\n",
      "Query ID: 51, Precision@10: 0.5\n",
      "Query ID:  52, Recall@10: 0.058823529411764705\n",
      "Query ID: 52, Precision@10: 0.4\n",
      "Query ID:  53, Recall@10: 0.0\n",
      "Query ID: 53, Precision@10: 0.0\n",
      "Query ID:  54, Recall@10: 0.06666666666666667\n",
      "Query ID: 54, Precision@10: 0.5\n",
      "Query ID:  55, Recall@10: 0.0\n",
      "Query ID: 55, Precision@10: 0.0\n",
      "Query ID:  56, Recall@10: 0.09302325581395349\n",
      "Query ID: 56, Precision@10: 0.8\n",
      "Query ID:  57, Recall@10: 0.030864197530864196\n",
      "Query ID: 57, Precision@10: 0.5\n",
      "Query ID:  58, Recall@10: 0.0661764705882353\n",
      "Query ID: 58, Precision@10: 0.9\n",
      "Query ID:  59, Recall@10: 0.017094017094017096\n",
      "Query ID: 59, Precision@10: 0.6\n",
      "Query ID:  60, Recall@10: 0.04522613065326633\n",
      "Query ID: 60, Precision@10: 0.9\n",
      "Query ID:  61, Recall@10: 0.005263157894736842\n",
      "Query ID: 61, Precision@10: 0.1\n",
      "Query ID:  62, Recall@10: 0.03272727272727273\n",
      "Query ID: 62, Precision@10: 0.9\n",
      "Query ID:  63, Recall@10: 0.045454545454545456\n",
      "Query ID: 63, Precision@10: 0.2\n",
      "Query ID:  64, Recall@10: 0.014778325123152709\n",
      "Query ID: 64, Precision@10: 0.3\n",
      "Query ID:  65, Recall@10: 0.0\n",
      "Query ID: 65, Precision@10: 0.0\n",
      "Query ID:  66, Recall@10: 0.020833333333333332\n",
      "Query ID: 66, Precision@10: 0.1\n",
      "Query ID:  67, Recall@10: 0.050955414012738856\n",
      "Query ID: 67, Precision@10: 0.8\n",
      "Query ID:  68, Recall@10: 0.012195121951219513\n",
      "Query ID: 68, Precision@10: 0.2\n",
      "Query ID:  69, Recall@10: 0.06329113924050633\n",
      "Query ID: 69, Precision@10: 0.5\n",
      "Query ID:  70, Recall@10: 0.030303030303030304\n",
      "Query ID: 70, Precision@10: 0.7\n",
      "Query ID:  71, Recall@10: 0.0\n",
      "Query ID: 71, Precision@10: 0.0\n",
      "Query ID:  72, Recall@10: 0.0989010989010989\n",
      "Query ID: 72, Precision@10: 0.9\n",
      "Query ID:  73, Recall@10: 0.0\n",
      "Query ID: 73, Precision@10: 0.0\n",
      "Query ID:  74, Recall@10: 0.38461538461538464\n",
      "Query ID: 74, Precision@10: 0.5\n",
      "Query ID:  75, Recall@10: 0.020527859237536656\n",
      "Query ID: 75, Precision@10: 0.7\n",
      "Mean Average Precision (MAP@10): 0.6147323003275385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save and load functions for TF-IDF data\n",
    "def save_file(file_location: str, content):\n",
    "    if os.path.exists(file_location):\n",
    "        os.remove(file_location)\n",
    "    with open(file_location, 'wb') as handle:\n",
    "        pickle.dump(content, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "def load_file(file_location: str):\n",
    "    with open(file_location, 'rb') as handle:\n",
    "        content = pickle.load(handle)\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_tfidf_data(tfidf_matrix, tfidf_model):\n",
    "    save_file(os.path.join(\"D:\\ir-search-engine\\storage\", f\"clinicaltrials_tfidf_matrix.pickle\"), tfidf_matrix)\n",
    "    save_file(os.path.join(\"D:\\ir-search-engine\\storage\", f\"clinicaltrials_tfidf_model.pickle\"), tfidf_model)\n",
    "\n",
    "\n",
    "save_tfidf_data(tfidf_matrix, tfidf_model)\n",
    "\n",
    "\n",
    "\n",
    "def process_query(query: str, tfidf_model, tfidf_matrix):\n",
    "    query_tfidf = tfidf_model.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
    "    ranked_doc_indices = cosine_similarities.argsort()[::-1]\n",
    "    return ranked_doc_indices, cosine_similarities\n",
    "\n",
    "\n",
    "\n",
    "tfidf_matrix = load_file(\"D:\\ir-search-engine\\storage\\\\clinicaltrials_tfidf_matrix.pickle\")\n",
    "tfidf_model = load_file(\"D:\\ir-search-engine\\storage\\\\clinicaltrials_tfidf_model.pickle\")\n",
    "\n",
    "\n",
    "\n",
    "def getRetrievedQueries(query: str, k=10):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    ranked_indices, _ = process_query(preprocessed_query, tfidf_model, tfidf_matrix)\n",
    "    idsList = []\n",
    "    for idx in ranked_indices[:k]:\n",
    "        doc_id = list(corpus.keys())[idx]\n",
    "        idsList.append(doc_id)\n",
    "    return idsList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_recall_precision(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break  \n",
    "\n",
    "    y_true = [1 if doc_id in relevant_docs else 0 for doc_id in retrieved_docs]\n",
    "    true_positives = sum(y_true)\n",
    "    recall_at_10 = true_positives / len(relevant_docs) if relevant_docs else 0\n",
    "    precision_at_10 = true_positives / 10\n",
    "    print(f\"Query ID:  {query_id}, Recall@10: {recall_at_10}\")\n",
    "    print(f\"Query ID: {query_id}, Precision@10: {precision_at_10}\")    \n",
    "    return recall_at_10\n",
    "\n",
    "\n",
    "\n",
    "queries_ids = {qrel[0]: '' for qrel in dataset.qrels_iter()}\n",
    "\n",
    "\n",
    "\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    calculate_recall_precision(query_id)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_MAP(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "\n",
    "    total_relevant = 0\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "\n",
    "        for j in range(i):\n",
    "            if j < len(retrieved_docs) and retrieved_docs[j] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs else 0)\n",
    "\n",
    "        pk_sum += p_at_k\n",
    "\n",
    "        if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "\n",
    "queries_ids = {qrel[0]: '' for qrel in dataset.qrels_iter()}\n",
    "\n",
    "map_sum = 0\n",
    "\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    map_sum += calculate_MAP(query_id)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Mean Average Precision (MAP@10): {map_sum / len(queries_ids)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank (MRR): 0.6773650793650796\n"
     ]
    }
   ],
   "source": [
    "def calculate_MRR(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        if retrieved_docs[i-1] in relevant_docs:\n",
    "            return 1 / i\n",
    "      \n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "queries_list = list(queries_ids.keys())\n",
    "mrr_sum = 0\n",
    "for query_id in queries_list:\n",
    "    mrr_sum += calculate_MRR(query_id)\n",
    "print(f\"Mean Reciprocal Rank (MRR): {(1 / len(queries_list)) * mrr_sum}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
