{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import ir_datasets\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import unicodedata\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# %load_ext autotime\n",
    "dataset = ir_datasets.load(\"beir/webis-touche2020/v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess the corpus\n",
    "corpus = {}\n",
    "for doc in dataset.docs_iter()[:1000]:\n",
    "    corpus[doc.doc_id] = doc.title + \" \" + doc.text + \" \" + doc.stance +\" \"+ doc.url\n",
    "\n",
    "documents = list(corpus.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Initialize the Google Translate API translator\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_english(text):\n",
    "    # Translate the text to English\n",
    "    translation = translator.translate(text, dest='en')\n",
    "    return translation.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    " \n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(word_tokenize(str(sentence)))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def remove_urls(data):\n",
    "    cleaned_text = re.sub(r'/(https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z]{2,}(\\.[a-zA-Z]{2,})(\\.[a-zA-Z]{2,})?\\/[a-zA-Z0-9]{2,}|((https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z]{2,}(\\.[a-zA-Z]{2,})(\\.[a-zA-Z]{2,})?)|(https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z0-9]{2,}\\.[a-zA-Z0-9]{2,}\\.[a-zA-Z0-9]{2,}(\\.[a-zA-Z0-9]{2,})?/g', '', data)\n",
    "    return cleaned_text;\n",
    "\n",
    "def replace_contractions(data):\n",
    "    return \" \".join(contractions.fix(data))\n",
    "\n",
    "def correct_sentence_spelling(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in misspelled:\n",
    "            corrected = spell.correction(token)\n",
    "            if corrected is not None:\n",
    "                tokens[i] = corrected\n",
    "    return \" \".join( tokens)\n",
    "\n",
    "\n",
    "\n",
    "def replace_country_symbols(data):\n",
    "   \n",
    "    country_symbols = {\n",
    "    'AF': 'Afghanistan',\n",
    "    'AX': 'Åland Islands',\n",
    "    'AL': 'Albania',\n",
    "    'DZ': 'Algeria',\n",
    "    'AS': 'American Samoa',\n",
    "    'AD': 'Andorra',\n",
    "    'AO': 'Angola',\n",
    "    'AI': 'Anguilla',\n",
    "    'AQ': 'Antarctica',\n",
    "    'AG': 'Antigua and Barbuda',\n",
    "    'AR': 'Argentina',\n",
    "    'AM': 'Armenia',\n",
    "    'AW': 'Aruba',\n",
    "    'AU': 'Australia',\n",
    "    'AT': 'Austria',\n",
    "    'AZ': 'Azerbaijan',\n",
    "    'BS': 'Bahamas',\n",
    "    'BH': 'Bahrain',\n",
    "    'BD': 'Bangladesh',\n",
    "    'BB': 'Barbados',\n",
    "    'BY': 'Belarus',\n",
    "    'BE': 'Belgium',\n",
    "    'BZ': 'Belize',\n",
    "    'BJ': 'Benin',\n",
    "    'BM': 'Bermuda',\n",
    "    'BT': 'Bhutan',\n",
    "    'BO': 'Bolivia',\n",
    "    'BQ': 'Bonaire, Sint Eustatius and Saba',\n",
    "    'BA': 'Bosnia and Herzegovina',\n",
    "    'BW': 'Botswana',\n",
    "    'BV': 'Bouvet Island',\n",
    "    'BR': 'Brazil',\n",
    "    'IO': 'British Indian Ocean Territory',\n",
    "    'BN': 'Brunei Darussalam',\n",
    "    'BG': 'Bulgaria',\n",
    "    'BF': 'Burkina Faso',\n",
    "    'BI': 'Burundi',\n",
    "    'CV': 'Cabo Verde',\n",
    "    'KH': 'Cambodia',\n",
    "    'CM': 'Cameroon',\n",
    "    'CA': 'Canada',\n",
    "    'KY': 'Cayman Islands',\n",
    "    'CF': 'Central African Republic',\n",
    "    'TD': 'Chad',\n",
    "    'CL': 'Chile',\n",
    "    'CN': 'China',\n",
    "    'CX': 'Christmas Island',\n",
    "    'CC': 'Cocos (Keeling) Islands',\n",
    "    'CO': 'Colombia',\n",
    "    'KM': 'Comoros',\n",
    "    'CG': 'Congo',\n",
    "    'CD': 'Congo, Democratic Republic of the',\n",
    "    'CK': 'Cook Islands',\n",
    "    'CR': 'Costa Rica',\n",
    "    'CI': 'Côte d\\'Ivoire',\n",
    "    'HR': 'Croatia',\n",
    "    'CU': 'Cuba',\n",
    "    'CW': 'Curaçao',\n",
    "    'CY': 'Cyprus',\n",
    "    'CZ': 'Czech Republic',\n",
    "    'DK': 'Denmark',\n",
    "    'DJ': 'Djibouti',\n",
    "    'DM': 'Dominica',\n",
    "    'DO': 'Dominican Republic',\n",
    "    'EC': 'Ecuador',\n",
    "    'EG': 'Egypt',\n",
    "    'SV': 'El Salvador',\n",
    "    'GQ': 'Equatorial Guinea',\n",
    "    'ER': 'Eritrea',\n",
    "    'EE': 'Estonia',\n",
    "    'SZ': 'Eswatini',\n",
    "    'ET': 'Ethiopia',\n",
    "    'FK': 'Falkland Islands (Malvinas)',\n",
    "    'FO': 'Faroe Islands',\n",
    "    'FJ': 'Fiji',\n",
    "    'FI': 'Finland',\n",
    "    'FR': 'France',\n",
    "    'GF': 'French Guiana',\n",
    "    'PF': 'French Polynesia',\n",
    "    'TF': 'French Southern Territories',\n",
    "    'GA': 'Gabon',\n",
    "    'GM': 'Gambia',\n",
    "    'GE': 'Georgia',\n",
    "    'DE': 'Germany',\n",
    "    'GH': 'Ghana',\n",
    "    'GI': 'Gibraltar',\n",
    "    'GR': 'Greece',\n",
    "    'GL': 'Greenland',\n",
    "    'GD': 'Grenada',\n",
    "    'GP': 'Guadeloupe',\n",
    "    'GU': 'Guam',\n",
    "    'GT': 'Guatemala',\n",
    "    'GG': 'Guernsey',\n",
    "    'GN': 'Guinea',\n",
    "    'GW': 'Guinea-Bissau',\n",
    "    'GY': 'Guyana',\n",
    "    'HT': 'Haiti',\n",
    "    'HM': 'Heard Island and McDonald Islands',\n",
    "    'VA': 'Holy See',\n",
    "    'HN': 'Honduras',\n",
    "    'HK': 'Hong Kong',\n",
    "    'HU': 'Hungary',\n",
    "    'IS': 'Iceland',\n",
    "    'IN': 'India',\n",
    "    'ID': 'Indonesia',\n",
    "    'IR': 'Iran, Islamic Republic of',\n",
    "    'IQ': 'Iraq',\n",
    "    'IE': 'Ireland',\n",
    "    'IM': 'Isle of Man',\n",
    "    'IL': 'Israel',\n",
    "    'IT': 'Italy',\n",
    "    'JM': 'Jamaica',\n",
    "    'JP': 'Japan',\n",
    "    'JE': 'Jersey',\n",
    "    'JO': 'Jordan',\n",
    "    'KZ': 'Kazakhstan',\n",
    "    'KE': 'Kenya',\n",
    "    'KI': 'Kiribati',\n",
    "    'KP': \"Korea, Democratic People's Republic of\",\n",
    "    'KR': 'Korea, Republic of',\n",
    "    'KW': 'Kuwait',\n",
    "    'KG': 'Kyrgyzstan',\n",
    "    'LA': \"Lao People's Democratic Republic\",\n",
    "    'LV': 'Latvia',\n",
    "    'LB': 'Lebanon',\n",
    "    'LS': 'Lesotho',\n",
    "    'LR': 'Liberia',\n",
    "    'LY': 'Libya',\n",
    "    'LI': 'Liechtenstein',\n",
    "    'LT': 'Lithuania',\n",
    "    'LU': 'Luxembourg',\n",
    "    'MO': 'Macao',\n",
    "    'MG': 'Madagascar',\n",
    "    'MW': 'Malawi',\n",
    "    'MY': 'Malaysia',\n",
    "    'MV': 'Maldives',\n",
    "    'ML': 'Mali',\n",
    "    'MT': 'Malta',\n",
    "    'MH': 'Marshall Islands',\n",
    "    'MQ': 'Martinique',\n",
    "    'MR': 'Mauritania',\n",
    "    'MU': 'Mauritius',\n",
    "    'YT': 'Mayotte',\n",
    "    'MX': 'Mexico',\n",
    "    'FM': 'Micronesia, Federated States of',\n",
    "    'MD': 'Moldova, Republic of',\n",
    "    'MC': 'Monaco',\n",
    "    'MN': 'Mongolia',\n",
    "    'ME': 'Montenegro',\n",
    "    'MS': 'Montserrat',\n",
    "    'MA': 'Morocco',\n",
    "    'MZ': 'Mozambique',\n",
    "    'MM': 'Myanmar',\n",
    "    'NA': 'Namibia',\n",
    "    'NR': 'Nauru',\n",
    "    'NP': 'Nepal',\n",
    "    'NL': 'Netherlands',\n",
    "    'NC': 'New Caledonia',\n",
    "    'NZ': 'New Zealand',\n",
    "    'NI': 'Nicaragua',\n",
    "    'NE': 'Niger',\n",
    "    'NG': 'Nigeria',\n",
    "    'NU': 'Niue',\n",
    "    'NF': 'Norfolk Island',\n",
    "    'MK': 'North Macedonia',\n",
    "    'MP': 'Northern Mariana Islands',\n",
    "    'NO': 'Norway',\n",
    "    'OM': 'Oman',\n",
    "    'PK': 'Pakistan',\n",
    "    'PW': 'Palau',\n",
    "    'PS': 'Palestine, State of',\n",
    "    'PA': 'Panama',\n",
    "    'PG': 'Papua New Guinea',\n",
    "    'PY': 'Paraguay',\n",
    "    'PE': 'Peru',\n",
    "    'PH': 'Philippines',\n",
    "    'PN': 'Pitcairn',\n",
    "    'PL': 'Poland',\n",
    "    'PT': 'Portugal',\n",
    "    'PR': 'Puerto Rico',\n",
    "    'QA': 'Qatar',\n",
    "    'RE': 'Réunion',\n",
    "    'RO': 'Romania',\n",
    "    'RU': 'Russian Federation',\n",
    "    'RW': 'Rwanda',\n",
    "    'BL': 'Saint Barthélemy',\n",
    "    'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n",
    "    'KN': 'Saint Kitts and Nevis',\n",
    "    'LC': 'Saint Lucia',\n",
    "    'MF': 'Saint Martin (French part)',\n",
    "    'PM': 'Saint Pierre and Miquelon',\n",
    "    'VC': 'Saint Vincent and the Grenadines',\n",
    "    'WS': 'Samoa',\n",
    "    'SM': 'San Marino',\n",
    "    'ST': 'Sao Tome and Principe',\n",
    "    'SA': 'Saudi Arabia',\n",
    "    'SN': 'Senegal',\n",
    "    'RS': 'Serbia',\n",
    "    'SC': 'Seychelles',\n",
    "    'SL': 'Sierra Leone',\n",
    "    'SG': 'Singapore',\n",
    "    'SX': 'Sint Maarten (Dutch part)',\n",
    "    'SK': 'Slovakia',\n",
    "    'SI': 'Slovenia',\n",
    "    'SB': 'Solomon Islands',\n",
    "    'SO': 'Somalia',\n",
    "    'ZA': 'South Africa',\n",
    "    'GS': 'South Georgia and the South Sandwich Islands',\n",
    "    'SS': 'South Sudan',\n",
    "    'ES': 'Spain',\n",
    "    'LK': 'Sri Lanka',\n",
    "    'SD': 'Sudan',\n",
    "    'SR': 'Suriname',\n",
    "    'SJ': 'Svalbard and Jan Mayen',\n",
    "    'SE': 'Sweden',\n",
    "    'CH': 'Switzerland',\n",
    "    'SY': 'Syrian Arab Republic',\n",
    "    'TW': 'Taiwan, Province of China',\n",
    "    'TJ': 'Tajikistan',\n",
    "    'TZ': 'Tanzania, United Republic of',\n",
    "    'TH': 'Thailand',\n",
    "    'TL': 'Timor-Leste',\n",
    "    'TG': 'Togo',\n",
    "    'TK': 'Tokelau',\n",
    "    'TO': 'Tonga',\n",
    "    'TT': 'Trinidad and Tobago',\n",
    "    'TN': 'Tunisia',\n",
    "    'TR': 'Turkey',\n",
    "    'TM': 'Turkmenistan',\n",
    "    'TC': 'Turks and Caicos Islands',\n",
    "    'TV': 'Tuvalu',\n",
    "    'UG': 'Uganda',\n",
    "    'UA': 'Ukraine',\n",
    "    'AE': 'United Arab Emirates',\n",
    "    'GB': 'United Kingdom',\n",
    "    'US': 'United States',\n",
    "    'UM': 'United States Minor Outlying Islands',\n",
    "    'UY': 'Uruguay',\n",
    "    'UZ': 'Uzbekistan',\n",
    "    'VU': 'Vanuatu',\n",
    "    'VE': 'Venezuela, Bolivarian Republic of',\n",
    "    'VN': 'Viet Nam',\n",
    "    'VG': 'Virgin Islands, British',\n",
    "    'VI': 'Virgin Islands, U.S.',\n",
    "    'WF': 'Wallis and Futuna',\n",
    "    'EH': 'Western Sahara',\n",
    "    'YE': 'Yemen',\n",
    "    'ZM': 'Zambia',\n",
    "    'ZW': 'Zimbabwe'\n",
    "    }\n",
    "    words = word_tokenize(data)\n",
    "    replaced_words = [country_symbols[word] if word in country_symbols else word for word in words]\n",
    "    return \" \".join(replaced_words)\n",
    "\n",
    "def custom_tokenizer(text: str) -> List[str]:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def preprocess(data):\n",
    "    data = replace_country_symbols(data)\n",
    "    data = remove_urls(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) \n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess each document and store the results in a new list\n",
    "preprocessed_documents = [preprocess(doc) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new TfidfVectorizer instance\n",
    "new_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed documents\n",
    "new_tfidf_matrix = new_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Create a DataFrame from the new TF-IDF matrix\n",
    "new_df = pd.DataFrame.sparse.from_spmatrix(new_tfidf_matrix, columns=new_vectorizer.get_feature_names_out(), index=corpus.keys())\n",
    "\n",
    "# Update the tfidf_model variable\n",
    "new_tfidf_model = new_vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load functions for TF-IDF data\n",
    "def save_file(file_location: str, content):\n",
    "    if os.path.exists(file_location):\n",
    "        os.remove(file_location)\n",
    "    with open(file_location, 'wb') as handle:\n",
    "        pickle.dump(content, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_file(file_location: str):\n",
    "    with open(file_location, 'rb') as handle:\n",
    "        content = pickle.load(handle)\n",
    "    return content\n",
    "\n",
    "# Save the new TF-IDF data\n",
    "def save_tfidf_data(tfidf_matrix, tfidf_model):\n",
    "    save_file(\"tfidf_matrix.pickle\", tfidf_matrix)\n",
    "    save_file(\"tfidf_model.pickle\", tfidf_model)\n",
    "\n",
    "save_tfidf_data(new_tfidf_matrix, new_tfidf_model)\n",
    "\n",
    "# Load the new TF-IDF matrix and model\n",
    "new_tfidf_matrix = load_file(\"tfidf_matrix.pickle\")\n",
    "new_tfidf_model = load_file(\"tfidf_model.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to process queries with the new TF-IDF data\n",
    "def process_query(query: str, tfidf_model, tfidf_matrix):\n",
    "    query_tfidf = tfidf_model.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
    "    ranked_doc_indices = cosine_similarities.argsort()[::-1]\n",
    "    return ranked_doc_indices, cosine_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to retrieve queries based on the preprocessed query\n",
    "def getRetrievedQueries(query: str, k=10):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    ranked_indices, _ = process_query(preprocessed_query, new_tfidf_model, new_tfidf_matrix)\n",
    "    idsList = []\n",
    "    for idx in ranked_indices[:k]:\n",
    "        doc_id = list(corpus.keys())[idx]\n",
    "        idsList.append(doc_id)\n",
    "    return idsList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_recall_precision(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break  \n",
    "\n",
    "    y_true = [1 if doc_id in relevant_docs else 0 for doc_id in retrieved_docs]\n",
    "    true_positives = sum(y_true)\n",
    "    recall_at_10 = true_positives / len(relevant_docs) if relevant_docs else 0\n",
    "    precision_at_10 = true_positives / 10\n",
    "    print(f\"Query ID:  {query_id}, Recall@10: {recall_at_10}\")\n",
    "    print(f\"Query ID: {query_id}, Precision@10: {precision_at_10}\")    \n",
    "    return recall_at_10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID:  1, Recall@10: 0.0\n",
      "Query ID: 1, Precision@10: 0.0\n",
      "Query ID:  2, Recall@10: 0.0\n",
      "Query ID: 2, Precision@10: 0.0\n",
      "Query ID:  3, Recall@10: 0.0\n",
      "Query ID: 3, Precision@10: 0.0\n",
      "Query ID:  4, Recall@10: 0.0\n",
      "Query ID: 4, Precision@10: 0.0\n",
      "Query ID:  5, Recall@10: 0.0\n",
      "Query ID: 5, Precision@10: 0.0\n",
      "Query ID:  6, Recall@10: 0.0\n",
      "Query ID: 6, Precision@10: 0.0\n",
      "Query ID:  7, Recall@10: 0.0\n",
      "Query ID: 7, Precision@10: 0.0\n",
      "Query ID:  8, Recall@10: 0.0\n",
      "Query ID: 8, Precision@10: 0.0\n",
      "Query ID:  9, Recall@10: 0.0\n",
      "Query ID: 9, Precision@10: 0.0\n",
      "Query ID:  10, Recall@10: 0.0\n",
      "Query ID: 10, Precision@10: 0.0\n",
      "Query ID:  11, Recall@10: 0.0\n",
      "Query ID: 11, Precision@10: 0.0\n",
      "Query ID:  12, Recall@10: 0.0\n",
      "Query ID: 12, Precision@10: 0.0\n",
      "Query ID:  13, Recall@10: 0.0\n",
      "Query ID: 13, Precision@10: 0.0\n",
      "Query ID:  14, Recall@10: 0.0\n",
      "Query ID: 14, Precision@10: 0.0\n",
      "Query ID:  15, Recall@10: 0.0\n",
      "Query ID: 15, Precision@10: 0.0\n",
      "Query ID:  16, Recall@10: 0.0\n",
      "Query ID: 16, Precision@10: 0.0\n",
      "Query ID:  17, Recall@10: 0.0\n",
      "Query ID: 17, Precision@10: 0.0\n",
      "Query ID:  18, Recall@10: 0.0\n",
      "Query ID: 18, Precision@10: 0.0\n",
      "Query ID:  19, Recall@10: 0.0\n",
      "Query ID: 19, Precision@10: 0.0\n",
      "Query ID:  20, Recall@10: 0.0\n",
      "Query ID: 20, Precision@10: 0.0\n",
      "Query ID:  21, Recall@10: 0.0\n",
      "Query ID: 21, Precision@10: 0.0\n",
      "Query ID:  22, Recall@10: 0.0\n",
      "Query ID: 22, Precision@10: 0.0\n",
      "Query ID:  23, Recall@10: 0.0\n",
      "Query ID: 23, Precision@10: 0.0\n",
      "Query ID:  24, Recall@10: 0.0\n",
      "Query ID: 24, Precision@10: 0.0\n",
      "Query ID:  26, Recall@10: 0.0\n",
      "Query ID: 26, Precision@10: 0.0\n",
      "Query ID:  27, Recall@10: 0.038461538461538464\n",
      "Query ID: 27, Precision@10: 0.1\n",
      "Query ID:  28, Recall@10: 0.0\n",
      "Query ID: 28, Precision@10: 0.0\n",
      "Query ID:  29, Recall@10: 0.0\n",
      "Query ID: 29, Precision@10: 0.0\n",
      "Query ID:  30, Recall@10: 0.0\n",
      "Query ID: 30, Precision@10: 0.0\n",
      "Query ID:  31, Recall@10: 0.0\n",
      "Query ID: 31, Precision@10: 0.0\n",
      "Query ID:  32, Recall@10: 0.0\n",
      "Query ID: 32, Precision@10: 0.0\n",
      "Query ID:  33, Recall@10: 0.0\n",
      "Query ID: 33, Precision@10: 0.0\n",
      "Query ID:  34, Recall@10: 0.0\n",
      "Query ID: 34, Precision@10: 0.0\n",
      "Query ID:  35, Recall@10: 0.0\n",
      "Query ID: 35, Precision@10: 0.0\n",
      "Query ID:  36, Recall@10: 0.0\n",
      "Query ID: 36, Precision@10: 0.0\n",
      "Query ID:  37, Recall@10: 0.0\n",
      "Query ID: 37, Precision@10: 0.0\n",
      "Query ID:  38, Recall@10: 0.0\n",
      "Query ID: 38, Precision@10: 0.0\n",
      "Query ID:  39, Recall@10: 0.0\n",
      "Query ID: 39, Precision@10: 0.0\n",
      "Query ID:  40, Recall@10: 0.0\n",
      "Query ID: 40, Precision@10: 0.0\n",
      "Query ID:  41, Recall@10: 0.0\n",
      "Query ID: 41, Precision@10: 0.0\n",
      "Query ID:  42, Recall@10: 0.0\n",
      "Query ID: 42, Precision@10: 0.0\n",
      "Query ID:  43, Recall@10: 0.0\n",
      "Query ID: 43, Precision@10: 0.0\n",
      "Query ID:  44, Recall@10: 0.0\n",
      "Query ID: 44, Precision@10: 0.0\n",
      "Query ID:  45, Recall@10: 0.0\n",
      "Query ID: 45, Precision@10: 0.0\n",
      "Query ID:  46, Recall@10: 0.0\n",
      "Query ID: 46, Precision@10: 0.0\n",
      "Query ID:  47, Recall@10: 0.0\n",
      "Query ID: 47, Precision@10: 0.0\n",
      "Query ID:  48, Recall@10: 0.0\n",
      "Query ID: 48, Precision@10: 0.0\n",
      "Query ID:  49, Recall@10: 0.0\n",
      "Query ID: 49, Precision@10: 0.0\n",
      "Query ID:  50, Recall@10: 0.0\n",
      "Query ID: 50, Precision@10: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queries_ids = {qrel[0]: '' for qrel in dataset.qrels_iter()}\n",
    "\n",
    "\n",
    "\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    calculate_recall_precision(query_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_MAP(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "\n",
    "    total_relevant = 0\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "\n",
    "        for j in range(i):\n",
    "            if j < len(retrieved_docs) and retrieved_docs[j] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs else 0)\n",
    "\n",
    "        pk_sum += p_at_k\n",
    "\n",
    "        if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MRR(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        if retrieved_docs[i-1] in relevant_docs:\n",
    "            return 1 / i\n",
    "      \n",
    "\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP@10): 0.02040816326530612\n"
     ]
    }
   ],
   "source": [
    "queries_ids = {qrel[0]: '' for qrel in dataset.qrels_iter()}\n",
    "\n",
    "map_sum = 0\n",
    "\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    map_sum += calculate_MAP(query_id)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Mean Average Precision (MAP@10): {map_sum / len(queries_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank (MRR): 0.02040816326530612\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queries_list = list(queries_ids.keys())\n",
    "mrr_sum = 0\n",
    "for query_id in queries_list:\n",
    "    mrr_sum += calculate_MRR(query_id)\n",
    "print(f\"Mean Reciprocal Rank (MRR): {(1 / len(queries_list)) * mrr_sum}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
